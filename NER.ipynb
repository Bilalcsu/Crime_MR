{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb73f740-740a-44c1-b4f7-9f1367af5e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded for NER. Number of rows: 782871\n",
      "Columns available: ['Report Number', 'Report DateTime', 'Offense ID', 'Offense Date', 'NIBRS Group AB', 'NIBRS Crime Against Category', 'Offense Sub Category', 'Shooting Type Group', 'Block Address', 'Latitude', 'Longitude', 'Beat', 'Precinct', 'Sector', 'Neighborhood', 'Reporting Area', 'Offense Category', 'NIBRS Offense Code Description', 'NIBRS_offense_code', 'Report DateTime_std', 'Offense Date_std', 'Offense_Description_Clean']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load libraries and preprocessed dataset\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load the preprocessed dataset saved in Step 1\n",
    "df = pd.read_csv(\"data/crime_data_nlppreprocessed.csv\")\n",
    "df.columns = df.columns.str.strip()  # ensure no spaces\n",
    "\n",
    "print(\"Dataset loaded for NER. Number of rows:\", len(df))\n",
    "print(\"Columns available:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e5afba3-9f23-4b20-8160-f49abb943d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy NER model loaded.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load spaCy NLP model (for NER)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"spaCy NER model loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b702222-7327-4280-9d68-6e5feaaf4cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Define function to extract entities from cleaned text\n",
    "def extract_entities(text):\n",
    "    \"\"\"\n",
    "    Extracts PERSON, GPE (locations), and potential weapons from text.\n",
    "    Returns a dictionary of entities.\n",
    "    \"\"\"\n",
    "    entities = {\"persons\": [], \"locations\": [], \"weapons\": []}\n",
    "    \n",
    "    if pd.isna(text) or text.strip() == \"\":\n",
    "        return entities\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Extract PERSON and GPE entities\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            entities[\"persons\"].append(ent.text)\n",
    "        elif ent.label_ in [\"GPE\", \"LOC\"]:\n",
    "            entities[\"locations\"].append(ent.text)\n",
    "    \n",
    "    # Simple weapon extraction using keywords (domain-specific)\n",
    "    weapon_keywords = [\"gun\", \"knife\", \"firearm\", \"weapon\", \"bat\", \"knife\", \"rifle\", \"pistol\"]\n",
    "    for token in doc:\n",
    "        if token.text.lower() in weapon_keywords:\n",
    "            entities[\"weapons\"].append(token.text.lower())\n",
    "    \n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8933aee5-745a-4404-88e0-8efa12cdaf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity extraction complete. Sample entities:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Offense_Description_Clean</th>\n",
       "      <th>Entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>intimidation</td>\n",
       "      <td>{'persons': [], 'locations': [], 'weapons': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Burglary Breaking Entering</td>\n",
       "      <td>{'persons': [], 'locations': [], 'weapons': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>false Pretenses swindle confidence Game</td>\n",
       "      <td>{'persons': [], 'locations': [], 'weapons': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>theft Motor Vehicle</td>\n",
       "      <td>{'persons': ['Motor Vehicle'], 'locations': []...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Larceny</td>\n",
       "      <td>{'persons': [], 'locations': [], 'weapons': []}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Offense_Description_Clean  \\\n",
       "0                             intimidation   \n",
       "1               Burglary Breaking Entering   \n",
       "2  false Pretenses swindle confidence Game   \n",
       "3                      theft Motor Vehicle   \n",
       "4                                  Larceny   \n",
       "\n",
       "                                            Entities  \n",
       "0    {'persons': [], 'locations': [], 'weapons': []}  \n",
       "1    {'persons': [], 'locations': [], 'weapons': []}  \n",
       "2    {'persons': [], 'locations': [], 'weapons': []}  \n",
       "3  {'persons': ['Motor Vehicle'], 'locations': []...  \n",
       "4    {'persons': [], 'locations': [], 'weapons': []}  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Apply NER extraction using nlp.pipe for efficiency\n",
    "texts = df['Offense_Description_Clean'].fillna(\"\").tolist()\n",
    "entities_list = []\n",
    "\n",
    "for doc in nlp.pipe(texts, batch_size=1000, n_process=2):  # use 2 CPU cores\n",
    "    # Extract entities from each doc\n",
    "    entities = {\"persons\": [], \"locations\": [], \"weapons\": []}\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            entities[\"persons\"].append(ent.text)\n",
    "        elif ent.label_ in [\"GPE\", \"LOC\"]:\n",
    "            entities[\"locations\"].append(ent.text)\n",
    "    \n",
    "    # Weapon keywords\n",
    "    weapon_keywords = [\"gun\", \"knife\", \"firearm\", \"weapon\", \"bat\", \"rifle\", \"pistol\"]\n",
    "    for token in doc:\n",
    "        if token.text.lower() in weapon_keywords:\n",
    "            entities[\"weapons\"].append(token.text.lower())\n",
    "    \n",
    "    entities_list.append(entities)\n",
    "\n",
    "# Store extracted entities in the dataframe\n",
    "df['Entities'] = entities_list\n",
    "print(\"Entity extraction complete. Sample entities:\")\n",
    "df[['Offense_Description_Clean', 'Entities']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67adf3f0-f5c8-408f-9546-6e7e67f05e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity-extracted dataset saved as 'data/crime_data_entities.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Save dataset with extracted entities\n",
    "df.to_csv(\"data/crime_data_entities.csv\", index=False)\n",
    "print(\"Entity-extracted dataset saved as 'data/crime_data_entities.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f752d025-1c97-45b0-8675-dfa9ce4ca762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with entities. Number of rows: 782871\n",
      "Columns: ['Report Number', 'Report DateTime', 'Offense ID', 'Offense Date', 'NIBRS Group AB', 'NIBRS Crime Against Category', 'Offense Sub Category', 'Shooting Type Group', 'Block Address', 'Latitude', 'Longitude', 'Beat', 'Precinct', 'Sector', 'Neighborhood', 'Reporting Area', 'Offense Category', 'NIBRS Offense Code Description', 'NIBRS_offense_code', 'Report DateTime_std', 'Offense Date_std', 'Offense_Description_Clean', 'Entities']\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Category_Clean'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDataset loaded with entities. Number of rows:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df))\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mColumns:\u001b[39m\u001b[33m\"\u001b[39m, df.columns.tolist())\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mOffense_Description_Clean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mEntities\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCategory_Clean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mBlock Address\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mNeighborhood\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mOffense Date_std\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Crime_MR\\Lib\\site-packages\\pandas\\core\\frame.py:4119\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4118\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4119\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4121\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Crime_MR\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6212\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6209\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6210\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6212\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6214\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6216\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\Crime_MR\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6264\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6261\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6263\u001b[39m not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m-> \u001b[39m\u001b[32m6264\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"['Category_Clean'] not in index\""
     ]
    }
   ],
   "source": [
    "# Step 3.1 – Load the dataset with extracted entities\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/crime_data_entities.csv\")\n",
    "df.columns = df.columns.str.strip()  # clean column names\n",
    "\n",
    "# Quick inspection\n",
    "print(\"Dataset loaded with entities. Number of rows:\", len(df))\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "df[['Offense_Description_Clean', 'Entities', 'Category_Clean', 'Block Address', 'Neighborhood', 'Offense Date_std']].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa261090-0012-4a07-bb84-0ecd6057050d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
